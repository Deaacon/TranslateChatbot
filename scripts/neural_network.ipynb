{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронная сеть для проекта"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\roman\\Documents\\Study\\Major\\Junior\\Semester_2\\Интеллектуальная обработка данных\\Проект\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from subword_nmt.learn_bpe import learn_bpe\n",
    "from subword_nmt.apply_bpe import BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x, tokenizer):\n",
    "    return ' '.join(tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:12<00:00, 666.65it/s] \n",
      "100%|██████████| 8000/8000 [00:33<00:00, 236.50it/s] \n"
     ]
    }
   ],
   "source": [
    "# загружаем данные и токенизатор\n",
    "data = tensorflow_datasets.load('ted_hrlr_translate/ru_to_en')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "# разбиваем и токенизируем тексты, записываем обработанные токены в файл\n",
    "for lang in ['en', 'ru']:\n",
    "    with open('../data/train.' + lang, 'w', encoding='utf-8') as f_out:\n",
    "        for line in data['train']:\n",
    "            src_line = line[lang].numpy().decode('utf8')\n",
    "            f_out.write(tokenize(src_line, tokenizer) + '\\n')\n",
    "\n",
    "# строим и применяем bpe кодирование\n",
    "bpe = {}\n",
    "for lang in ['en', 'ru']:\n",
    "    learn_bpe(open('../data/train.' + lang, encoding='utf-8'), open('../data/bpe_rules.' + lang, 'w', encoding='utf-8'), num_symbols=8000)\n",
    "    bpe[lang] = BPE(open('../data/bpe_rules.' + lang, encoding='utf-8'))\n",
    "    \n",
    "    with open('../data/train.bpe.' + lang, 'w', encoding='utf-8') as f_out:\n",
    "        for line in open('../data/train.' + lang, encoding='utf-8'):\n",
    "            f_out.write(bpe[lang].process_line(line.strip()) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from vocab import Vocab\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_inp = np.array(open('../data/train.bpe.ru', encoding='utf-8').read().split('\\n'))\n",
    "data_out = np.array(open('../data/train.bpe.en', encoding='utf-8').read().split('\\n'))\n",
    "\n",
    "train_inp, dev_inp, train_out, dev_out = train_test_split(data_inp, data_out, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp: не поступа@@ йте так !\n",
      "out: do not do that .\n",
      "\n",
      "inp: ( смех в зале ) а потом она на меня посмотре@@ ла .\n",
      "out: ( laughter ) and then she looked at me .\n",
      "\n",
      "inp: в центре g@@ al@@ l@@ u@@ p я изуча@@ ю мусуль@@ ман@@ ские сообщества во всем мире .\n",
      "out: i study muslim societies around the world at gall@@ up .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('inp:', train_inp[i])\n",
    "    print('out:', train_out[i], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "inp_voc = Vocab.from_lines(train_inp)\n",
    "out_voc = Vocab.from_lines(train_out)\n",
    "\n",
    "with open('../data/voc.ru', 'wb') as file:\n",
    "    pickle.dump(inp_voc, file)\n",
    "with open('../data/voc.en', 'wb') as file:\n",
    "    pickle.dump(out_voc, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines\n",
      "['не поступа@@ йте так !'\n",
      " '( смех в зале ) а потом она на меня посмотре@@ ла .'\n",
      " 'в центре g@@ al@@ l@@ u@@ p я изуча@@ ю мусуль@@ ман@@ ские сообщества во всем мире .']\n",
      "\n",
      "words to ids (0 = bos, 1 = eos):\n",
      "tensor([[   0, 3805, 5006, 2591, 6816,    3,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1],\n",
      "        [   0,   11, 6255,  656, 2141,   12,  297, 5021, 4285, 3621, 3366, 4984,\n",
      "         3003,   19,    1,    1,    1,    1,    1,    1],\n",
      "        [   0,  656, 7610,  156,  121,  183,  237,  206, 8124, 2401, 8090, 3593,\n",
      "         3284, 6136, 6405,  976, 1127, 3451,   19,    1]])\n",
      "\n",
      "back to words\n",
      "['не поступа@@ йте так !', '( смех в зале ) а потом она на меня посмотре@@ ла .', 'в центре g@@ al@@ l@@ u@@ p я изуча@@ ю мусуль@@ ман@@ ские сообщества во всем мире .']\n"
     ]
    }
   ],
   "source": [
    "# тут можно посмотреть, как работает мапинг из индекса в токен и наоборот\n",
    "batch_lines = train_inp[:3]\n",
    "batch_ids = inp_voc.to_matrix(batch_lines)\n",
    "batch_lines_restored = inp_voc.to_lines(batch_ids)\n",
    "\n",
    "print(\"lines\")\n",
    "print(batch_lines)\n",
    "print(\"\\nwords to ids (0 = bos, 1 = eos):\")\n",
    "print(batch_ids)\n",
    "print(\"\\nback to words\")\n",
    "print(batch_lines_restored)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from functional import loss_function, compute_bleu\n",
    "from model import AttentiveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/10000 [00:26<9:18:30,  3.35s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     18\u001b[0m loss \u001b[39m=\u001b[39m loss_function(model, batch_inp, batch_out)\n\u001b[1;32m---> 19\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     20\u001b[0m opt\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     21\u001b[0m metrics[\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend((i, loss\u001b[39m.\u001b[39mitem()))\n",
      "File \u001b[1;32md:\\roman\\Documents\\Study\\Major\\Junior\\Semester_2\\Интеллектуальная обработка данных\\Проект\\.venv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32md:\\roman\\Documents\\Study\\Major\\Junior\\Semester_2\\Интеллектуальная обработка данных\\Проект\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "metrics = {'train_loss': [], 'dev_bleu': [] }\n",
    "\n",
    "model = AttentiveModel(inp_voc, out_voc).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "\n",
    "num_iter = 10000\n",
    "\n",
    "for i in tqdm(range(num_iter)):\n",
    "    batch_indices = np.random.randint(len(train_inp), size=batch_size)\n",
    "    batch_lines_inp = train_inp[batch_indices]\n",
    "    batch_lines_out = train_out[batch_indices]\n",
    "    batch_inp = inp_voc.to_matrix(batch_lines_inp).to(device)\n",
    "    batch_out = out_voc.to_matrix(batch_lines_out).to(device)\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss = loss_function(model, batch_inp, batch_out)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    metrics['train_loss'].append((i, loss.item()))\n",
    "    if i % 100 == 0:\n",
    "        bleu = compute_bleu(model, batch_lines_inp, batch_lines_out)\n",
    "        metrics['dev_bleu'].append((i, bleu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "для этого не нужно быть полностью сле@@ пы@@ м , достаточно суще@@ ственного расстрой@@ ства зрения .\n",
      ", , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "но несмотря на то , что мы узнали о болезни сравни@@ тельно недавно и не владе@@ ем ис@@ чер@@ пы@@ ва@@ ющей информацией , мы знаем , как её остановить .\n",
      ", , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n",
      "« о чём ты , эли@@ н ? » « да ни о чём особен@@ ном . о том , о с@@ ём , о ра@@ е и а@@ де . давайте подни@@ ме@@ мся на кры@@ шу .\n",
      ", , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , ,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for inp_line, trans_line in zip(dev_inp[:3], model.translate_lines(dev_inp[:3])[0]):\n",
    "    print(inp_line, trans_line, sep='\\n', end='\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
